{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmXQV9eONqVk"
      },
      "source": [
        "# Run Consistory Batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/consistory.git"
      ],
      "metadata": {
        "id": "JM3A17GyZ9NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd consistory"
      ],
      "metadata": {
        "id": "9mrBSC55aJtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "dLEd8W3yeXrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ../requirements.txt"
      ],
      "metadata": {
        "id": "lPtre8-y0ePn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft==0.6.2"
      ],
      "metadata": {
        "id": "XnkE3jmaeh8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLtqq0cNqVm"
      },
      "outputs": [],
      "source": [
        "# Copyright (C) 2024 NVIDIA Corporation.  All rights reserved.\n",
        "#\n",
        "# This work is licensed under the LICENSE file\n",
        "# located at the root directory.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from consistory_run import load_pipeline, run_batch_generation\n",
        "\n",
        "gpu = 0\n",
        "story_pipeline = load_pipeline(gpu)\n",
        "\n",
        "style = \"A photo of \"\n",
        "subject = \"a cute dog\"\n",
        "concept_token = ['dog']\n",
        "settings = [\"sitting in the beach\",\n",
        "            \"standing in the snow\",\n",
        "            \"playing in the park\"]\n",
        "\n",
        "seed = 40\n",
        "mask_dropout = 0.5\n",
        "same_latent = False\n",
        "n_achors = 2\n",
        "\n",
        "prompts = [f'{style}{subject} {setting}' for setting in settings]\n",
        "\n",
        "# Reset the GPU memory tracking\n",
        "torch.cuda.reset_max_memory_allocated(gpu)\n",
        "\n",
        "images, image_all = run_batch_generation(story_pipeline, prompts, concept_token, seed, mask_dropout=mask_dropout, same_latent=same_latent, n_achors = 2)\n",
        "display(image_all)\n",
        "\n",
        "# Report maximum GPU memory usage in GB\n",
        "max_memory_used = torch.cuda.max_memory_allocated(gpu) / (1024**3)  # Convert to GB\n",
        "print(f\"Maximum GPU memory used: {max_memory_used:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSik1wxtNqVo"
      },
      "source": [
        "# Run Consistory w/ Cached Anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Oe26ku5NqVo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from consistory_run import load_pipeline, run_anchor_generation, run_extra_generation\n",
        "\n",
        "gpu = 0\n",
        "story_pipeline = load_pipeline(gpu)\n",
        "\n",
        "style = \"A photo of \"\n",
        "subject = \"a cute dog\"\n",
        "concept_token = ['dog']\n",
        "anchor_settings = [\"sitting in the beach\", \"standing in the snow\"]\n",
        "extra_settings = [\"playing in the park\", \"surfing in the ocean\"]\n",
        "\n",
        "seed = 40\n",
        "mask_dropout = 0.5\n",
        "same_latent = False\n",
        "\n",
        "anchor_prompts = [f'{style}{subject} {setting}' for setting in anchor_settings]\n",
        "extra_prompts = [f'{style}{subject} {setting}' for setting in extra_settings]\n",
        "\n",
        "# Reset the GPU memory tracking\n",
        "torch.cuda.reset_max_memory_allocated(gpu)\n",
        "\n",
        "anchor_out_images, anchor_image_all, anchor_cache_first_stage, anchor_cache_second_stage = run_anchor_generation(story_pipeline, anchor_prompts, concept_token,\n",
        "                                                                                                       seed=seed, mask_dropout=mask_dropout, same_latent=same_latent,\n",
        "                                                                                                       cache_cpu_offloading=True)\n",
        "\n",
        "print('Anchor images:')\n",
        "display(anchor_image_all)\n",
        "\n",
        "for extra_prompt in extra_prompts:\n",
        "    extra_out_images, extra_image_all = run_extra_generation(story_pipeline, [extra_prompt], concept_token, anchor_cache_first_stage, anchor_cache_second_stage,\n",
        "                                                seed=seed, mask_dropout=mask_dropout, same_latent=same_latent, cache_cpu_offloading=True)\n",
        "\n",
        "    print(f'Extra prompt: {extra_prompt}')\n",
        "    display(extra_image_all)\n",
        "\n",
        "# Report maximum GPU memory usage in GB\n",
        "max_memory_used = torch.cuda.max_memory_allocated(gpu) / (1024**3)  # Convert to GB\n",
        "print(f\"Maximum GPU memory used: {max_memory_used:.2f} GB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}